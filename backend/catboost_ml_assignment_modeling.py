# -*- coding: utf-8 -*-
"""CatBoost - ML Assignment Modeling.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19QaUcehMC4zyQnpNIJyxtMmk0JoF8IAA
"""




import pandas as pd
import numpy as np
from xgboost import XGBRegressor
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer
from sklearn.metrics import mean_absolute_error, r2_score,mean_squared_error
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

dataset_path = 'data/datasets/car_price_dataset.csv'

df = pd.read_csv(dataset_path)

if 'Unnamed: 0' in df.columns:
    df.drop(columns=['Unnamed: 0'], inplace=True)
    print("Dropped 'Unnamed: 0'")

# Display the first 5 rows of the DataFrame
print("First 5 rows of the DataFrame:")
df.head()

import xgboost
print(f"XGBoost Version: {xgboost.__version__}")



"""# Pre Cleanup - Outlier Removal"""

yom_threshold = 1970
mileage_threshold = 500_000  # km

# VERY OLD CARS REMOVAL
old_cars_mask = df['YOM'] < yom_threshold
print(f"1. Very Old Cars (YOM < {yom_threshold}):")
print(f"   Records to remove: {old_cars_mask.sum():,} ({old_cars_mask.sum()/len(df)*100:.2f}%)")

# EXTREME MILEAGE REMOVAL
extreme_mileage_mask = df['Millage(KM)'] > mileage_threshold


combined_outlier_mask = old_cars_mask | extreme_mileage_mask

print(f"Total outliers to remove: {combined_outlier_mask.sum():,} ({combined_outlier_mask.sum()/len(df)*100:.2f}%)")
print(f"Records retained: {(~combined_outlier_mask).sum():,} ({(~combined_outlier_mask).sum()/len(df)*100:.2f}%)")

df = df[~combined_outlier_mask].copy()

"""# Custom Transformer"""

def create_car_age(X):
    """Create Car_Age from YOM"""
    X = X.copy()
    X['Car_Age'] = 2025 - X['YOM']
    return X

def create_log_mileage(X):
    """Create Mileage_log from Millage(KM)"""
    X = X.copy()
    X['Mileage_log'] = np.log1p(X['Millage(KM)'])
    return X

def group_brands(X):
    """Group brands to top 15 + Other"""
    X = X.copy()
    top_brands = ['TOYOTA', 'SUZUKI', 'NISSAN', 'HONDA', 'MITSUBISHI',
                  'PERODUA', 'MICRO', 'HYUNDAI', 'MAZDA', 'MERCEDES-BENZ',
                  'TATA', 'KIA', 'DAIHATSU', 'BMW', 'RENAULT']
    X['Brand_Grouped'] = X['Brand'].apply(
        lambda x: x if x in top_brands else 'Other'
    )
    return X

def group_fuel(X):
    """Group fuel types"""
    X = X.copy()
    X['Fuel_Grouped'] = X['Fuel Type'].replace({
        'Electric': 'Alternative',
        'Hybrid': 'Alternative'
    })
    return X

# def create_mileage_segments(X):
#     """Create mileage segments"""
#     X = X.copy()
#     X['Mileage_Segment'] = pd.cut(
#         X['Millage(KM)'],
#         bins=[0, 150000, 300000, 1_000_000],
#         labels=['Low', 'Medium', 'High']
#     )
#     return X

def encode_binary_features(X):
    """Encode binary features"""
    X = X.copy()
    binary_mappings = {
        'Gear': {'Automatic': 1, 'Manual': 0},
        'Leasing': {'Ongoing Lease': 1, 'No Leasing': 0},
        'Condition': {'NEW': 1, 'USED': 0},
        'AIR CONDITION': {'Available': 1, 'Not_Available': 0},
        'POWER STEERING': {'Available': 1, 'Not_Available': 0},
        'POWER MIRROR': {'Available': 1, 'Not_Available': 0},
        'POWER WINDOW': {'Available': 1, 'Not_Available': 0}
    }

    for col, mapping in binary_mappings.items():
        if col in X.columns:
            X[col] = X[col].map(mapping)

    return X

def remove_outliers(X):
    """Remove outliers"""
    X = X.copy()
    # Create Car_Age if not exists
    if 'Car_Age' not in X.columns and 'YOM' in X.columns:
        X['Car_Age'] = 2025 - X['YOM']

    # Filter outliers
    mask = (
        (X['Car_Age'] <= 30) &
        (X['Millage(KM)'] <= 500_000) &
        (X['Price'] <= 500)
    )
    return X[mask]

def select_features(X):
    """Drop unnecessary columns"""
    X = X.copy()
    drop_cols = ['YOM', 'Millage(KM)', 'Model', 'Town', 'Date',
                 'Brand', 'Fuel Type', 'Car_Age']  # Drop Car_Age if VIF high

    X.drop(columns=[c for c in drop_cols if c in X.columns],
           inplace=True, errors='ignore')
    return X

"""# Create Preprocessing Pipeline"""

preprocessing_pipeline = Pipeline([
    ('create_age', FunctionTransformer(create_car_age, validate=False)),
    ('create_log_mileage', FunctionTransformer(create_log_mileage, validate=False)),
    ('group_brands', FunctionTransformer(group_brands, validate=False)),
    ('group_fuel', FunctionTransformer(group_fuel, validate=False)),
    # ('create_mileage_segments', FunctionTransformer(create_mileage_segments, validate=False)),
    ('encode_binary', FunctionTransformer(encode_binary_features, validate=False)),
    ('select_features', FunctionTransformer(select_features, validate=False))
])

"""# Create ColumnTransformer"""

column_transformer = ColumnTransformer(
    transformers=[
        # Numerical features - just scale
        ('num_engine', StandardScaler(), ['Engine (cc)']),
        ('num_mileage', StandardScaler(), ['Mileage_log']),

        # Binary features - passthrough (already 0/1)
        ('binary', 'passthrough', [
            'Gear', 'Leasing', 'Condition',
            'AIR CONDITION', 'POWER STEERING', 'POWER MIRROR', 'POWER WINDOW'
        ]),

        # Categorical features - one-hot encode
        ('cat', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'), [
            'Brand_Grouped', 'Fuel_Grouped',
            # 'Mileage_Segment'
        ])
    ],
    remainder='drop'
)

"""# Prepare Data"""

df = pd.read_csv(dataset_path)

# REMOVE OUTLIERS FIRST (before split)
print("Removing outliers...")

# Drop unnamed column
if 'Unnamed: 0' in df.columns:
    df.drop(columns=['Unnamed: 0'], inplace=True)

# Create Car_Age for filtering
df['Car_Age_temp'] = 2025 - df['YOM']

# Apply outlier filters
df_clean = df[
    (df['Car_Age_temp'] <= 30) &
    (df['Millage(KM)'] <= 500_000) &
    (df['Price'] <= 500)
].copy()

# Drop temporary column
df_clean.drop(columns=['Car_Age_temp'], inplace=True)

print(f"After outlier removal: {df_clean.shape}")
print(f"Removed: {len(df) - len(df_clean):,} outliers")

X = df_clean.drop(columns=['Price'])
y = np.log1p(df_clean['Price'])

"""# Train Test Split

- Taining Set: 70%
- Testing Set: 30%
"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

print(f"\nðŸ“Š Training data shape: {X_train.shape}")
print(f"ðŸ“Š Testing data shape: {X_test.shape}")

"""# CatBoost Regressor Model

CatBoost is an ensemble technique that combines multiple weak learners (usually decision trees) to create a strong learner. It adjusts weights iteratively to focus on difficult-to-predict instances.
"""


from catboost import CatBoostRegressor
from sklearn.base import BaseEstimator, RegressorMixin

# Wrapper class to make CatBoost compatible with scikit-learn 1.6+
class CatBoostRegressorWrapper(BaseEstimator, RegressorMixin):
    """Wrapper for CatBoostRegressor to ensure sklearn compatibility."""

    def __init__(self, iterations=1000, learning_rate=0.05, depth=6,
                 l2_leaf_reg=3, loss_function='RMSE', eval_metric='RMSE',
                 random_seed=42, verbose=0, early_stopping_rounds=50):
        self.iterations = iterations
        self.learning_rate = learning_rate
        self.depth = depth
        self.l2_leaf_reg = l2_leaf_reg
        self.loss_function = loss_function
        self.eval_metric = eval_metric
        self.random_seed = random_seed
        self.verbose = verbose
        self.early_stopping_rounds = early_stopping_rounds
        self._model = None

    def fit(self, X, y):
        self._model = CatBoostRegressor(
            iterations=self.iterations,
            learning_rate=self.learning_rate,
            depth=self.depth,
            l2_leaf_reg=self.l2_leaf_reg,
            loss_function=self.loss_function,
            eval_metric=self.eval_metric,
            random_seed=self.random_seed,
            verbose=self.verbose,
            early_stopping_rounds=self.early_stopping_rounds
        )
        self._model.fit(X, y)
        # Mark as fitted for sklearn compatibility
        self.is_fitted_ = True
        return self

    def predict(self, X):
        return self._model.predict(X)

    def __sklearn_is_fitted__(self):
        """Check if the estimator is fitted."""
        return hasattr(self, 'is_fitted_') and self.is_fitted_

    def __sklearn_tags__(self):
        """Return sklearn tags for compatibility."""
        tags = super().__sklearn_tags__()
        return tags

    def get_params(self, deep=True):
        return {
            'iterations': self.iterations,
            'learning_rate': self.learning_rate,
            'depth': self.depth,
            'l2_leaf_reg': self.l2_leaf_reg,
            'loss_function': self.loss_function,
            'eval_metric': self.eval_metric,
            'random_seed': self.random_seed,
            'verbose': self.verbose,
            'early_stopping_rounds': self.early_stopping_rounds
        }

    def set_params(self, **params):
        for key, value in params.items():
            setattr(self, key, value)
        return self


# Create the full pipeline for CatBoost
catboost_pipeline = Pipeline([
    ('preprocessing', preprocessing_pipeline),
    ('column_transform', column_transformer),
    ('model', CatBoostRegressorWrapper(
        iterations=500,
        learning_rate=0.1,
        depth=8,
        l2_leaf_reg=1,
        loss_function='RMSE',
        eval_metric='RMSE',
        random_seed=42,
        verbose=0,
        early_stopping_rounds=50
    ))
])

print("CatBoost pipeline created!")

print("\nðŸš€ Training CatBoost pipeline...")
catboost_pipeline.fit(X_train, y_train)
print("âœ… Training complete!")

y_pred = catboost_pipeline.predict(X_test)

# Reverse the Log Transformation of the Target
y_test_actual = np.expm1(y_test)
y_pred_actual = np.expm1(y_pred)

"""### Evaluation (CatBoost)
Calculating MAE, RMSE, and R2 Score for CatBoost.
"""

# Predict
y_pred_cat = catboost_pipeline.predict(X_test)

# Reverse the Log Transformation
y_pred_actual_cat = np.expm1(y_pred_cat)

# Calculate Metrics
mae_cat = mean_absolute_error(y_test_actual, y_pred_actual_cat)
mse_cat = mean_squared_error(y_test_actual, y_pred_actual_cat)
rmse_cat = np.sqrt(mse_cat)
r2_cat = r2_score(y_test_actual, y_pred_actual_cat)

print("--- CatBoost Evaluation Results ---")
print(f"Mean Absolute Error (MAE): {mae_cat:.2f}")
print(f"Root Mean Squared Error (RMSE): {rmse_cat:.2f}")
print(f"R-squared (R2) Score: {r2_cat:.4f}")

# Visualization
plt.figure(figsize=(10, 6))
plt.scatter(y_test_actual, y_pred_actual_cat, alpha=0.5, color='orange')
plt.plot([y_test_actual.min(), y_test_actual.max()],
         [y_test_actual.min(), y_test_actual.max()],
         'r--', lw=2)
plt.xlabel('Actual Price')
plt.ylabel('Predicted Price')
plt.title('Actual vs Predicted Car Prices (catBoost)')
plt.show()



"""## Shap Analysis"""

import shap
import matplotlib.pyplot as plt

# Get the column transformer from pipeline
column_transformer = catboost_pipeline.named_steps['column_transform']

# Extract feature names from each transformer
feature_names_list = []

# 1. Numerical features (scaled)
num_engine_features = ['Engine (cc)']
num_mileage_features = ['Mileage_log']

# 2. Binary features (passthrough)
binary_features = [
    'Gear', 'Leasing', 'Condition',
    'AIR CONDITION', 'POWER STEERING', 'POWER MIRROR', 'POWER WINDOW'
]

# 3. Categorical features (one-hot encoded)
categorical_transformer = column_transformer.named_transformers_['cat']
categorical_features_input = ['Brand_Grouped', 'Fuel_Grouped',
                              # 'Mileage_Segment'
                              ]
categorical_features_encoded = categorical_transformer.get_feature_names_out(categorical_features_input)

# Combine all feature names in order
all_feature_names = (
    num_engine_features +
    num_mileage_features +
    binary_features +
    list(categorical_features_encoded)
)

print(f"\nTotal features after transformation: {len(all_feature_names)}")
print(f"\nFeature breakdown:")
print(f"  â€¢ Numerical (scaled): {len(num_engine_features) + len(num_mileage_features)}")
print(f"  â€¢ Binary (passthrough): {len(binary_features)}")
print(f"  â€¢ Categorical (one-hot): {len(categorical_features_encoded)}")

print(f"\n All feature names:")
for i, name in enumerate(all_feature_names, 1):
    print(f"  {i:2d}. {name}")

# ============================================
# STEP 2: TRANSFORM TEST DATA
# ============================================

print("\nðŸ” Transforming test data for SHAP...")

# Apply preprocessing + column transformation (everything except model)
X_test_processed = catboost_pipeline.named_steps['preprocessing'].transform(X_test)
X_test_processed = catboost_pipeline.named_steps['column_transform'].transform(X_test_processed)

# Convert to DataFrame with feature names
X_test_processed_df = pd.DataFrame(X_test_processed, columns=all_feature_names)

print(f"âœ… Transformed shape: {X_test_processed_df.shape}")

# ============================================
# STEP 3: CREATE SHAP EXPLAINER
# ============================================

print("\nðŸ” Creating SHAP explainer...")

# Get the model from pipeline (the CatBoostRegressor)
model = catboost_pipeline.named_steps['model']

# KernelExplainer can interpret any model that has a predict function, but it
# requires a background dataset (often a sample of the training data) to compute
# the expected value and calculate SHAP values.
print("  Preparing background data for KernelExplainer...")

# Apply preprocessing + column transformation to X_train to create the background dataset.
# This data will be used by KernelExplainer to estimate feature contributions.
X_train_processed = catboost_pipeline.named_steps['preprocessing'].transform(X_train)
X_train_processed = catboost_pipeline.named_steps['column_transform'].transform(X_train_processed)

# Convert the processed training data into a DataFrame with feature names.
# 'all_feature_names' was already determined and is available from a previous cell.
X_train_processed_df = pd.DataFrame(X_train_processed, columns=all_feature_names)

# For performance, select a small, representative sample from the processed training data
# to serve as the background dataset for KernelExplainer. A size of 100 samples is a common
# choice to balance accuracy with computational cost.
X_background_shap = X_train_processed_df.sample(min(100, len(X_train_processed_df)), random_state=42)

# Create KernelExplainer:
# - The first argument is the model's prediction function. Since our 'model' variable
#   is the CatBoostRegressor itself, we use 'model.predict'.
# - The second argument is the background dataset, 'X_background_shap', which consists
#   of processed training samples.
explainer = shap.KernelExplainer(model.predict, X_background_shap)

print("âœ… SHAP explainer (KernelExplainer) created!")

# ============================================
# STEP 4: CALCULATE SHAP VALUES (Subset for speed)
# ============================================

print("\nðŸ” Calculating SHAP values (using 100 samples)...")

# Use subset for speed
X_shap = X_test_processed_df.head(100)

# Calculate SHAP values
shap_values = explainer.shap_values(X_shap)

print(f"âœ… SHAP values calculated")
print(f"   Shape: {shap_values.shape}")

# ============================================
# STEP 5: VISUALIZATIONS
# ============================================

print("\nGenerating SHAP visualizations...")

# -------------------- Bar Plot (Feature Importance) --------------------
plt.figure(figsize=(10, 8))
shap.summary_plot(shap_values, X_shap, plot_type="bar", show=False)
plt.title('SHAP Feature Importance (Average Magnitude)', fontsize=14, fontweight='bold')
plt.xlabel('Mean |SHAP value|', fontsize=12)
plt.tight_layout()
plt.show()

"""According to feature importance plot:
- **Milage** contribute the mosts to ecide the price
- Second comes **Manual or Auto**
- Third the **Engine Capacity**
"""

# -------------------- Beeswarm Plot (Impact and Direction) --------------------
plt.figure(figsize=(10, 8))
shap.summary_plot(shap_values, X_shap, show=False)
plt.title('SHAP Summary Plot (Impact and Direction)', fontsize=14, fontweight='bold')
plt.xlabel('SHAP value (impact on model output)', fontsize=12)
plt.tight_layout()
plt.show()

"""This plot tells how each feature affects the price,
- Left side(negative) : Decreases price
- Righ side(positive): Increases price

Colors:
- Red: High value(eg: high milage)
- Blue: Low value(eg: low milage)

According to the plot:
1. **Milage_log** most contributed feature, blue dots are in right side means low milages increases the price and high milage red dots in left decreases the price.
2. **Gear**(manual(0) or auto(1)), red dots are in right which means auto gear vehicles have increased price and manual one the blue dots in left decreases the price. Not like first feature we can see clear seperation.
3. **Engine(cc)**, since the red dots are on right side meand if the engine capacity increases the price increases and if the engine capacity decreases the price decreases

Real Example:
- Low mileage (50k km)    â†’ Push price UP
- Automatic               â†’ Push price UP
- Large engine (2000cc)   â†’ Push price UP
"""

# ============================================
# STEP 6: FEATURE IMPORTANCE RANKING
# ============================================

print("\n" + "="*70)
print("TOP 10 MOST IMPORTANT FEATURES")
print("="*70)

# Calculate mean absolute SHAP values for each feature
mean_abs_shap = np.abs(shap_values).mean(axis=0)

# Create DataFrame for sorting
feature_importance = pd.DataFrame({
    'Feature': all_feature_names,
    'Importance': mean_abs_shap
}).sort_values('Importance', ascending=False)

# Display top 10
for i, row in feature_importance.head(10).iterrows():
    print(f"{row.name+1:2d}. {row['Feature']:<40s} {row['Importance']:.4f}")

"""```
TOP 5 FEATURES:
1. Mileage_log          0.2230 (22.3%)
2. Gear                 0.1913 (19.1%)
3. Engine (cc)          0.1397 (14.0%)
4. Brand_TOYOTA         0.0656 (6.6%)
5. Mileage_Segment_Low  0.0643 (6.4%)
```
1. This also says Mileage contrbute more to the price more than any other features, to be exact 22.3% importance.
  ```
  Real Example:
  Car A: 50,000 km  â†’  Price: 60 lakhs
  Car B: 200,000 km â†’  Price: 40 lakhs
    
  Difference: 150,000 km = 20 lakhs difference!
  ```

2. Gear (Manual or Auto) is the second most important(19.13%)
  ```
  Real Example:
  Same car, same mileage, same year:
  
  Manual transmission:     45 lakhs
  Automatic transmission:  60 lakhs
  
  Difference: 15 lakhs just for being automatic!
  ```

3. Engine CC is the thirst most important(13.97%) feature.
  ```
  Real Example:
  1000cc (Alto):      30 lakhs
  1500cc (Corolla):   50 lakhs
  2500cc (Camry):     80 lakhs
  
  Bigger engine = Higher price
  ```

  4. Being Brand Toyota also contributes by 6.43%
    ```
    Real Example:
    Same specs, same year, same mileage:
  
  MICRO brand:    35 lakhs
  TOYOTA brand:   50 lakhs
  
  Just the "TOYOTA" name = +15 lakhs!
    ```
"""

# ============================================
# STEP 7: WATERFALL PLOT (Single Prediction)
# ============================================

print("\nGenerating waterfall plot for single prediction...")

# Select a single instance
instance_idx = 0
instance = X_shap.iloc[instance_idx:instance_idx+1]

# Create explanation object
shap_explanation = shap.Explanation(
    values=shap_values[instance_idx],
    base_values=explainer.expected_value,
    data=instance.values[0],
    feature_names=all_feature_names
)

# Waterfall plot
plt.figure(figsize=(10, 8))
shap.waterfall_plot(shap_explanation, show=False)
plt.title(f'SHAP Waterfall Plot - Prediction #{instance_idx}', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.show()

"""TODO
Shows step by step breakdown of One specific prediction.
In the Example,
- Base value: 4.029(~Rs. 40 lakh)
- Final value: 3.894(~Rs. 39 lakh)

Started with 40 lakh(average car price) then,

 -0.15 Nissan Brand price reduction â†’ ~ 38.5 lakh
 +0.14 Engine CC price increase â†’ ~39.9 lakh
 +0.11 Gear automatic trsansmission â†’ ~50 lakh
 -0.06 Mileage not low mileage â†’ ~49.4lakh
 -0.05 Not a Toyota brand â†’ ~48.9 lakh
 +0.03 Not a Zuzuki brand â†’ ~49.2 lakh
 +0.02 Fuel Group Petrol â†’ ~49.4 lakh
 +0.02 Power mirror â†’ ~49.6 lakh
 +0.02 Not a Micro brand â†’ ~49.8 lakh
"""

# ============================================
# STEP 8: FORCE PLOT (Interactive - optional)
# ============================================

print("\nðŸ“Š Generating force plot...")

# Force plot for first prediction
shap.initjs()
force_plot = shap.force_plot(
    explainer.expected_value,
    shap_values[instance_idx],
    X_shap.iloc[instance_idx],
    feature_names=all_feature_names
)


print("âœ… Force plot generated")

"""## Summary:

### Data Analysis Key Findings

*   The initial `ValueError` during SHAP analysis was successfully resolved by configuring the `OneHotEncoder` within the `ColumnTransformer` to produce dense output, specifically by setting `sparse_output=False`.
*   An `AttributeError` (`Estimator log does not provide get_feature_names_out.`) encountered when attempting to extract feature names for SHAP was addressed by directly utilizing a pre-defined `all_feature_names` variable to label the processed data, bypassing issues with `FunctionTransformer` not having the `get_feature_names_out()` method.
*   Following these corrections, the SHAP analysis successfully executed, generating both bar and beeswarm summary plots for feature importance and impact.

### Insights or Next Steps

*   When integrating preprocessing steps with advanced analysis tools like SHAP, ensure that data formats (e.g., dense vs. sparse) are compatible with the expectations of the analysis library.
*   Develop robust strategies for feature name management, especially when using custom transformers in pipelines, to prevent compatibility issues with downstream tools that require explicit feature labels.

# Save Pipeline
"""

import joblib

# Save the entire pipeline (Preprocessor + CatBoost)
joblib.dump(catboost_pipeline, 'data/models/car_price_predictor_catboost.pkl')


""" --- Later, in a different script or server ---

# Load the single file
model_service = joblib.load('car_price_predictor_full.pkl')

# You can now pass RAW data directly to it
raw_data = pd.DataFrame({
    'Brand': ['Toyota'],
    'Model': ['Corolla'],
    'Millage(KM)': [50000],
    'Engine (cc)': [1500],
    # ... other features ...
})

prediction = model_service.predict(raw_data)
"""

"""# Inference"""

# Load pipeline
pipeline_loaded = joblib.load('data/models/car_price_predictor_catboost.pkl')

# New car - RAW format
new_car = pd.DataFrame({
    'Brand': ['TOYOTA'],
    'Model': ['Corolla'],
    'YOM': [2018],
    'Engine (cc)': [1500],
    'Gear': ['Automatic'],
    'Fuel Type': ['Petrol'],
    'Millage(KM)': [120000],
    'Town': ['Colombo'],
    'Date': ['2025-01-10'],
    'Leasing': ['No Leasing'],
    'Condition': ['USED'],
    'AIR CONDITION': ['Available'],
    'POWER STEERING': ['Available'],
    'POWER MIRROR': ['Available'],
    'POWER WINDOW': ['Available']
})

print("\nðŸ“ Input:")
print(new_car[['Brand', 'YOM', 'Engine (cc)', 'Millage(KM)', 'Gear', 'Fuel Type']].T)

# Predict
price_log = pipeline_loaded.predict(new_car)[0]
price_lakhs = np.expm1(price_log)

print(f"\nðŸŽ¯ Predicted Price: {price_lakhs:.2f} lakhs")
print("\nâœ… Pipeline working perfectly!")

from sklearn.model_selection import GridSearchCV

# Define the parameter grid for CatBoost
param_grid = {
    'model__iterations': [100, 250, 500],  # Number of boosting iterations
    'model__learning_rate': [0.01, 0.05, 0.1], # Step size shrinkage
    'model__depth': [4, 6, 8], # Depth of the trees
    'model__l2_leaf_reg': [1, 3, 5] # L2 regularization coefficient
}

# Create a GridSearchCV object
# We use the existing catboost_pipeline, which includes preprocessing and column transformations
grid_search = GridSearchCV(
    estimator=catboost_pipeline,
    param_grid=param_grid,
    cv=3, # Number of folds for cross-validation
    scoring='neg_mean_absolute_error', # Metric to optimize (MAE is good for regression, negated for GridSearchCV)
    verbose=2, # Output progress
    n_jobs=-1 # Use all available cores
)

print("Starting GridSearchCV for hyperparameter tuning...")
# Fit GridSearchCV to the training data
grid_search.fit(X_train, y_train)

print("\nGridSearchCV complete!")

# Print the best parameters found
print("Best parameters found:", grid_search.best_params_)

# Print the best score (negated MAE, so convert back to positive MAE)
print("Best MAE (from cross-validation):", -grid_search.best_score_)

# Get the best model from GridSearchCV
best_catboost_pipeline = grid_search.best_estimator_

# Make predictions with the best model
y_pred_tuned = best_catboost_pipeline.predict(X_test)

# Reverse the log transformation
y_test_actual_tuned = np.expm1(y_test)
y_pred_actual_tuned = np.expm1(y_pred_tuned)

# Evaluate the best model
mae_tuned = mean_absolute_error(y_test_actual_tuned, y_pred_actual_tuned)
mse_tuned = mean_squared_error(y_test_actual_tuned, y_pred_actual_tuned)
rmse_tuned = np.sqrt(mse_tuned)
r2_tuned = r2_score(y_test_actual_tuned, y_pred_actual_tuned)

print("\n--- Tuned CatBoost Evaluation Results ---")
print(f"Mean Absolute Error (MAE): {mae_tuned:.2f}")
print(f"Root Mean Squared Error (RMSE): {rmse_tuned:.2f}")
print(f"R-squared (R2) Score: {r2_tuned:.4f}")

# Visualization for the tuned model
plt.figure(figsize=(10, 6))
plt.scatter(y_test_actual_tuned, y_pred_actual_tuned, alpha=0.5, color='green')
plt.plot([y_test_actual_tuned.min(), y_test_actual_tuned.max()],
         [y_test_actual_tuned.min(), y_test_actual_tuned.max()],
         'r--', lw=2)
plt.xlabel('Actual Price')
plt.ylabel('Predicted Price')
plt.title('Actual vs Predicted Car Prices (Tuned CatBoost)')
plt.show()


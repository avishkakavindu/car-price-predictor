# -*- coding: utf-8 -*-
"""Copy of LightGBM  Model - of ML Assignment Modeling.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10DuawxZEfeZ6vngEkWimhvzoewEHRBaL
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer
from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

dataset_path = 'data/datasets/car_price_dataset.csv'

df = pd.read_csv(dataset_path)


if 'Unnamed: 0' in df.columns:
    df.drop(columns=['Unnamed: 0'], inplace=True)
    print("Dropped 'Unnamed: 0'")

# Display the first 5 rows of the DataFrame
print("First 5 rows of the DataFrame:")
df.head()

"""# Custom Transformer"""

def create_car_age(X):
    """Create Car_Age from YOM"""
    X = X.copy()
    X['Car_Age'] = 2025 - X['YOM']
    return X

def create_log_mileage(X):
    """Create Mileage_log from Millage(KM)"""
    X = X.copy()
    X['Mileage_log'] = np.log1p(X['Millage(KM)'])
    return X

def group_brands(X):
    """Group brands to top 15 + Other"""
    X = X.copy()
    top_brands = ['TOYOTA', 'SUZUKI', 'NISSAN', 'HONDA', 'MITSUBISHI',
                  'PERODUA', 'MICRO', 'HYUNDAI', 'MAZDA', 'MERCEDES-BENZ',
                  'TATA', 'KIA', 'DAIHATSU', 'BMW', 'RENAULT']
    X['Brand_Grouped'] = X['Brand'].apply(
        lambda x: x if x in top_brands else 'Other'
    )
    return X

def group_fuel(X):
    """Group fuel types"""
    X = X.copy()
    X['Fuel_Grouped'] = X['Fuel Type'].replace({
        'Electric': 'Alternative',
        'Hybrid': 'Alternative'
    })
    return X

# def create_mileage_segments(X):
#     """Create mileage segments"""
#     X = X.copy()
#     X['Mileage_Segment'] = pd.cut(
#         X['Millage(KM)'],
#         bins=[0, 150000, 300000, 1_000_000],
#         labels=['Low', 'Medium', 'High']
#     )
#     return X

def encode_binary_features(X):
    """Encode binary features"""
    X = X.copy()
    binary_mappings = {
        'Gear': {'Automatic': 1, 'Manual': 0},
        'Leasing': {'Ongoing Lease': 1, 'No Leasing': 0},
        'Condition': {'NEW': 1, 'USED': 0},
        'AIR CONDITION': {'Available': 1, 'Not_Available': 0},
        'POWER STEERING': {'Available': 1, 'Not_Available': 0},
        'POWER MIRROR': {'Available': 1, 'Not_Available': 0},
        'POWER WINDOW': {'Available': 1, 'Not_Available': 0}
    }

    for col, mapping in binary_mappings.items():
        if col in X.columns:
            X[col] = X[col].map(mapping)

    return X

def remove_outliers(X):
    """Remove outliers"""
    X = X.copy()
    # Create Car_Age if not exists
    if 'Car_Age' not in X.columns and 'YOM' in X.columns:
        X['Car_Age'] = 2025 - X['YOM']

    # Filter outliers
    mask = (
        (X['Car_Age'] <= 30) &
        (X['Millage(KM)'] <= 500_000) &
        (X['Price'] <= 500)
    )
    return X[mask]

def select_features(X):
    """Drop unnecessary columns"""
    X = X.copy()
    drop_cols = ['YOM', 'Millage(KM)', 'Model', 'Town', 'Date',
                 'Brand', 'Fuel Type', 'Car_Age']  # Drop Car_Age if VIF high

    X.drop(columns=[c for c in drop_cols if c in X.columns],
           inplace=True, errors='ignore')
    return X

"""# Create Preprocessing Pipeline"""

preprocessing_pipeline = Pipeline([
    ('create_age', FunctionTransformer(create_car_age, validate=False)),
    ('create_log_mileage', FunctionTransformer(create_log_mileage, validate=False)),
    ('group_brands', FunctionTransformer(group_brands, validate=False)),
    ('group_fuel', FunctionTransformer(group_fuel, validate=False)),
    # ('create_mileage_segments', FunctionTransformer(create_mileage_segments, validate=False)),
    ('encode_binary', FunctionTransformer(encode_binary_features, validate=False)),
    ('select_features', FunctionTransformer(select_features, validate=False))
])

"""# Create ColumnTransformer"""

column_transformer = ColumnTransformer(
    transformers=[
        # Numerical features - just scale
        ('num_engine', StandardScaler(), ['Engine (cc)']),
        ('num_mileage', StandardScaler(), ['Mileage_log']),

        # Binary features - passthrough (already 0/1)
        ('binary', 'passthrough', [
            'Gear', 'Leasing', 'Condition',
            'AIR CONDITION', 'POWER STEERING', 'POWER MIRROR', 'POWER WINDOW'
        ]),

        # Categorical features - one-hot encode
        ('cat', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'), [
            'Brand_Grouped', 'Fuel_Grouped',
            # 'Mileage_Segment'
        ])
    ],
    remainder='drop'
)

"""# Prepare Data"""

df = pd.read_csv(dataset_path)

# REMOVE OUTLIERS FIRST (before split)
print("Removing outliers...")

# Drop unnamed column
if 'Unnamed: 0' in df.columns:
    df.drop(columns=['Unnamed: 0'], inplace=True)

# Create Car_Age for filtering
df['Car_Age_temp'] = 2025 - df['YOM']

# Apply outlier filters
df_clean = df[
    (df['Car_Age_temp'] <= 30) &
    (df['Millage(KM)'] <= 500_000) &
    (df['Price'] <= 500)
].copy()

# Drop temporary column
df_clean.drop(columns=['Car_Age_temp'], inplace=True)

print(f"After outlier removal: {df_clean.shape}")
print(f"Removed: {len(df) - len(df_clean):,} outliers")

X = df_clean.drop(columns=['Price'])
y = np.log1p(df_clean['Price'])

"""# Train Test Split

- Taining Set: 70%
- Testing Set: 30%
"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

print(f"\nðŸ“Š Training data shape: {X_train.shape}")
print(f"ðŸ“Š Testing data shape: {X_test.shape}")

"""# LightGBM Model

LightGBM is a gradient boosting framework that uses tree-based learning algorithms. It is designed to be distributed and efficient, often providing faster training speed and higher efficiency than standard gradient boosting.
"""

from lightgbm import LGBMRegressor

# Create the full pipeline for LightGBM
lgbm_pipeline = Pipeline([
    ('preprocessing', preprocessing_pipeline),
    ('column_transform', column_transformer),
    ('model', LGBMRegressor(
        n_estimators=1000,
        learning_rate=0.05,
        num_leaves=31,
        random_state=42,
        n_jobs=-1,
        verbose=-1
    ))
])

print("LightGBM pipeline created!")

print("\nðŸš€ Training LightGBM pipeline...")
lgbm_pipeline.fit(X_train, y_train)
print("âœ… Training complete!")

y_pred = lgbm_pipeline.predict(X_test)

# Reverse the Log Transformation of the Target
y_test_actual = np.expm1(y_test)
y_pred_actual = np.expm1(y_pred)

"""### Evaluation (LightGBM)
Calculating MAE, RMSE, and R2 Score for LightGBM.
"""

# Predict
y_pred_lgbm = lgbm_pipeline.predict(X_test)

# Reverse the Log Transformation
y_pred_actual_lgbm = np.expm1(y_pred_lgbm)

# Calculate Metrics
mae_lgbm = mean_absolute_error(y_test_actual, y_pred_actual_lgbm)
mse_lgbm = mean_squared_error(y_test_actual, y_pred_actual_lgbm)
rmse_lgbm = np.sqrt(mse_lgbm)
r2_lgbm = r2_score(y_test_actual, y_pred_actual_lgbm)

print("--- LightGBM Evaluation Results ---")
print(f"Mean Absolute Error (MAE): {mae_lgbm:.2f}")
print(f"Root Mean Squared Error (RMSE): {rmse_lgbm:.2f}")
print(f"R-squared (R2) Score: {r2_lgbm:.4f}")

# Visualization
plt.figure(figsize=(10, 6))
plt.scatter(y_test_actual, y_pred_actual_lgbm, alpha=0.5, color='purple')
plt.plot([y_test_actual.min(), y_test_actual.max()],
         [y_test_actual.min(), y_test_actual.max()],
         'r--', lw=2)
plt.xlabel('Actual Price')
plt.ylabel('Predicted Price')
plt.title('Actual vs Predicted Car Prices (LightGBM)')
plt.show()



"""## Shap Analysis"""

import shap
import matplotlib.pyplot as plt

# Get the column transformer from pipeline
column_transformer = lgbm_pipeline.named_steps['column_transform']

# Extract feature names from each transformer
feature_names_list = []

# 1. Numerical features (scaled)
num_engine_features = ['Engine (cc)']
num_mileage_features = ['Mileage_log']

# 2. Binary features (passthrough)
binary_features = [
    'Gear', 'Leasing', 'Condition',
    'AIR CONDITION', 'POWER STEERING', 'POWER MIRROR', 'POWER WINDOW'
]

# 3. Categorical features (one-hot encoded)
categorical_transformer = column_transformer.named_transformers_['cat']
categorical_features_input = ['Brand_Grouped', 'Fuel_Grouped',
                              # 'Mileage_Segment'
                              ]
categorical_features_encoded = categorical_transformer.get_feature_names_out(categorical_features_input)

# Combine all feature names in order
all_feature_names = (
    num_engine_features +
    num_mileage_features +
    binary_features +
    list(categorical_features_encoded)
)

print(f"\nTotal features after transformation: {len(all_feature_names)}")
print(f"\nFeature breakdown:")
print(f"  â€¢ Numerical (scaled): {len(num_engine_features) + len(num_mileage_features)}")
print(f"  â€¢ Binary (passthrough): {len(binary_features)}")
print(f"  â€¢ Categorical (one-hot): {len(categorical_features_encoded)}")

print(f"\n All feature names:")
for i, name in enumerate(all_feature_names, 1):
    print(f"  {i:2d}. {name}")

# ============================================
# STEP 2: TRANSFORM TEST DATA
# ============================================

print("\nðŸ” Transforming test data for SHAP...")

# Apply preprocessing + column transformation (everything except model)
X_test_processed = lgbm_pipeline.named_steps['preprocessing'].transform(X_test)
X_test_processed = lgbm_pipeline.named_steps['column_transform'].transform(X_test_processed)

# Convert to DataFrame with feature names
X_test_processed_df = pd.DataFrame(X_test_processed, columns=all_feature_names)

print(f"âœ… Transformed shape: {X_test_processed_df.shape}")

# ============================================
# STEP 3: CREATE SHAP EXPLAINER
# ============================================

print("\nðŸ” Creating SHAP explainer...")

# Get the model from pipeline
model = lgbm_pipeline.named_steps['model']

# Create TreeExplainer
explainer = shap.TreeExplainer(model)

print("âœ… SHAP explainer created")

# ============================================
# STEP 4: CALCULATE SHAP VALUES (Subset for speed)
# ============================================

print("\nðŸ” Calculating SHAP values (using 100 samples)...")

# Use subset for speed
X_shap = X_test_processed_df.head(100)

# Calculate SHAP values
shap_values = explainer.shap_values(X_shap)

print(f"âœ… SHAP values calculated")
print(f"   Shape: {shap_values.shape}")

# ============================================
# STEP 5: VISUALIZATIONS
# ============================================

print("\nGenerating SHAP visualizations...")

# -------------------- Bar Plot (Feature Importance) --------------------
plt.figure(figsize=(10, 8))
shap.summary_plot(shap_values, X_shap, plot_type="bar", show=False)
plt.title('SHAP Feature Importance (Average Magnitude)', fontsize=14, fontweight='bold')
plt.xlabel('Mean |SHAP value|', fontsize=12)
plt.tight_layout()
plt.show()

"""According to feature importance plot:
- **Milage** contribute the mosts to ecide the price
- Second comes **Manual or Auto**
- Third the **Engine Capacity**
"""

# -------------------- Beeswarm Plot (Impact and Direction) --------------------
plt.figure(figsize=(10, 8))
shap.summary_plot(shap_values, X_shap, show=False)
plt.title('SHAP Summary Plot (Impact and Direction)', fontsize=14, fontweight='bold')
plt.xlabel('SHAP value (impact on model output)', fontsize=12)
plt.tight_layout()
plt.show()

"""This plot tells how each feature affects the price,
- Left side(negative) : Decreases price
- Righ side(positive): Increases price

Colors:
- Red: High value(eg: high milage)
- Blue: Low value(eg: low milage)

According to the plot:
1. **Milage_log** most contributed feature, blue dots are in right side means low milages increases the price and high milage red dots in left decreases the price.
2. **Gear**(manual(0) or auto(1)), red dots are in right which means auto gear vehicles have increased price and manual one the blue dots in left decreases the price. Not like first feature we can see clear seperation.
3. **Engine(cc)**, since the red dots are on right side meand if the engine capacity increases the price increases and if the engine capacity decreases the price decreases

Real Example:
- Low mileage (50k km)    â†’ Push price UP
- Automatic               â†’ Push price UP
- Large engine (2000cc)   â†’ Push price UP
"""

# ============================================
# STEP 6: FEATURE IMPORTANCE RANKING
# ============================================

print("\n" + "="*70)
print("TOP 10 MOST IMPORTANT FEATURES")
print("="*70)

# Calculate mean absolute SHAP values for each feature
mean_abs_shap = np.abs(shap_values).mean(axis=0)

# Create DataFrame for sorting
feature_importance = pd.DataFrame({
    'Feature': all_feature_names,
    'Importance': mean_abs_shap
}).sort_values('Importance', ascending=False)

# Display top 10
for i, row in feature_importance.head(10).iterrows():
    print(f"{row.name+1:2d}. {row['Feature']:<40s} {row['Importance']:.4f}")

"""```
TOP 5 FEATURES:
1. Mileage_log          0.2230 (22.3%)
2. Gear                 0.1913 (19.1%)
3. Engine (cc)          0.1397 (14.0%)
4. Brand_TOYOTA         0.0656 (6.6%)
5. Mileage_Segment_Low  0.0643 (6.4%)
```
1. This also says Mileage contrbute more to the price more than any other features, to be exact 22.3% importance.
  ```
  Real Example:
  Car A: 50,000 km  â†’  Price: 60 lakhs
  Car B: 200,000 km â†’  Price: 40 lakhs
    
  Difference: 150,000 km = 20 lakhs difference!
  ```

2. Gear (Manual or Auto) is the second most important(19.13%)
  ```
  Real Example:
  Same car, same mileage, same year:
  
  Manual transmission:     45 lakhs
  Automatic transmission:  60 lakhs
  
  Difference: 15 lakhs just for being automatic!
  ```

3. Engine CC is the thirst most important(13.97%) feature.
  ```
  Real Example:
  1000cc (Alto):      30 lakhs
  1500cc (Corolla):   50 lakhs
  2500cc (Camry):     80 lakhs
  
  Bigger engine = Higher price
  ```

  4. Being Brand Toyota also contributes by 6.43%
    ```
    Real Example:
    Same specs, same year, same mileage:
  
  MICRO brand:    35 lakhs
  TOYOTA brand:   50 lakhs
  
  Just the "TOYOTA" name = +15 lakhs!
    ```
"""

# ============================================
# STEP 7: WATERFALL PLOT (Single Prediction)
# ============================================

print("\nGenerating waterfall plot for single prediction...")

# Select a single instance
instance_idx = 0
instance = X_shap.iloc[instance_idx:instance_idx+1]

# Create explanation object
shap_explanation = shap.Explanation(
    values=shap_values[instance_idx],
    base_values=explainer.expected_value,
    data=instance.values[0],
    feature_names=all_feature_names
)

# Waterfall plot
plt.figure(figsize=(10, 8))
shap.waterfall_plot(shap_explanation, show=False)
plt.title(f'SHAP Waterfall Plot - Prediction #{instance_idx}', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.show()

"""TODO
Shows step by step breakdown of One specific prediction.
In the Example,
- Base value: 4.029(~Rs. 40 lakh)
- Final value: 3.894(~Rs. 39 lakh)

Started with 40 lakh(average car price) then,

 -0.15 Nissan Brand price reduction â†’ ~ 38.5 lakh
 +0.14 Engine CC price increase â†’ ~39.9 lakh
 +0.11 Gear automatic trsansmission â†’ ~50 lakh
 -0.06 Mileage not low mileage â†’ ~49.4lakh
 -0.05 Not a Toyota brand â†’ ~48.9 lakh
 +0.03 Not a Zuzuki brand â†’ ~49.2 lakh
 +0.02 Fuel Group Petrol â†’ ~49.4 lakh
 +0.02 Power mirror â†’ ~49.6 lakh
 +0.02 Not a Micro brand â†’ ~49.8 lakh
"""

# ============================================
# STEP 8: FORCE PLOT (Interactive - optional)
# ============================================

print("\nðŸ“Š Generating force plot...")

# Force plot for first prediction
shap.initjs()
force_plot = shap.force_plot(
    explainer.expected_value,
    shap_values[instance_idx],
    X_shap.iloc[instance_idx],
    feature_names=all_feature_names
)


print("âœ… Force plot generated")

"""## Summary:

### Data Analysis Key Findings

*   The initial `ValueError` during SHAP analysis was successfully resolved by configuring the `OneHotEncoder` within the `ColumnTransformer` to produce dense output, specifically by setting `sparse_output=False`.
*   An `AttributeError` (`Estimator log does not provide get_feature_names_out.`) encountered when attempting to extract feature names for SHAP was addressed by directly utilizing a pre-defined `all_feature_names` variable to label the processed data, bypassing issues with `FunctionTransformer` not having the `get_feature_names_out()` method.
*   Following these corrections, the SHAP analysis successfully executed, generating both bar and beeswarm summary plots for feature importance and impact.

### Insights or Next Steps

*   When integrating preprocessing steps with advanced analysis tools like SHAP, ensure that data formats (e.g., dense vs. sparse) are compatible with the expectations of the analysis library.
*   Develop robust strategies for feature name management, especially when using custom transformers in pipelines, to prevent compatibility issues with downstream tools that require explicit feature labels.

# Save Pipeline

# Inference
"""

import joblib

dump_path ='data/models/car_price_predictor_lgbm.pkl'
# Save the entire pipeline (Preprocessor + XGBoost)
joblib.dump(lgbm_pipeline, dump_path)

""" --- Later, in a different script or server ---

# Load the single file
model_service = joblib.load('car_price_predictor_full.pkl')

# You can now pass RAW data directly to it
raw_data = pd.DataFrame({
    'Brand': ['Toyota'],
    'Model': ['Corolla'],
    'Millage(KM)': [50000],
    'Engine (cc)': [1500],
    # ... other features ...
})

prediction = model_service.predict(raw_data)
"""

# Load pipeline
pipeline_loaded = joblib.load(dump_path)

# New car - RAW format
new_car = pd.DataFrame({
    'Brand': ['TOYOTA'],
    'Model': ['Corolla'],
    'YOM': [2018],
    'Engine (cc)': [1500],
    'Gear': ['Automatic'],
    'Fuel Type': ['Petrol'],
    'Millage(KM)': [120000],
    'Town': ['Colombo'],
    'Date': ['2025-01-10'],
    'Leasing': ['No Leasing'],
    'Condition': ['USED'],
    'AIR CONDITION': ['Available'],
    'POWER STEERING': ['Available'],
    'POWER MIRROR': ['Available'],
    'POWER WINDOW': ['Available']
})

print("\nðŸ“ Input:")
print(new_car[['Brand', 'YOM', 'Engine (cc)', 'Millage(KM)', 'Gear', 'Fuel Type']].T)

# Predict
price_log = pipeline_loaded.predict(new_car)[0]
price_lakhs = np.expm1(price_log)

print(f"\nðŸŽ¯ Predicted Price: {price_lakhs:.2f} lakhs")
print("\nâœ… Pipeline working perfectly!")